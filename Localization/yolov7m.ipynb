{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCb3MKYeuM7c",
        "outputId": "14e9ae60-958e-449a-98d1-4931f64dc144"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"yolov7m_single.py\n",
        "\n",
        "YOLOv7m for Bangla License Plate Detection\n",
        "Converted from YOLOv5m implementation\n",
        "\"\"\"\n",
        "\n",
        "# Cell 1: Install dependencies and setup for YOLOv7\n",
        "import os\n",
        "\n",
        "# Clone YOLOv7 if not already cloned\n",
        "if not os.path.exists('/content/yolov7'):\n",
        "    !git clone https://github.com/WongKinYiu/yolov7.git /content/yolov7\n",
        "    print(\"YOLOv7 cloned successfully!\")\n",
        "else:\n",
        "    print(\"YOLOv7 already exists!\")\n",
        "\n",
        "# %cd /content/yolov7\n",
        "\n",
        "# Install required packages directly (no requirements.txt needed)\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q matplotlib>=3.2.2\n",
        "!pip install -q numpy>=1.18.5\n",
        "!pip install -q opencv-python>=4.1.2\n",
        "!pip install -q Pillow>=7.1.2\n",
        "!pip install -q PyYAML>=5.3.1\n",
        "!pip install -q scipy>=1.4.1\n",
        "!pip install -q torch>=1.7.0\n",
        "!pip install -q torchvision>=0.8.1\n",
        "!pip install -q tqdm>=4.41.0\n",
        "!pip install -q tensorboard>=2.4.1\n",
        "!pip install -q pandas>=1.1.4\n",
        "!pip install -q seaborn>=0.11.0\n",
        "\n",
        "# Additional packages\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Additional imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "print(\"All dependencies installed successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Cell 2: Mount Drive and setup paths\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "local_dataset_path = '/content/Bangla_License_Plate_Dataset'\n",
        "yolo_dataset_path = '/content/yolo_dataset'\n",
        "drive_backup_path = '/content/drive/MyDrive/Bangla_License_Plate_Dataset'\n",
        "\n",
        "print(\"Drive mounted successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryDEkMpGvdOU",
        "outputId": "06f6e705-80e8-43f0-cbca-6d2df6694064"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Download and prepare dataset (same as original)\n",
        "dataset_ready = False\n",
        "\n",
        "if os.path.exists(local_dataset_path):\n",
        "    print(\"Dataset already exists locally\")\n",
        "    dataset_ready = True\n",
        "elif os.path.exists(drive_backup_path):\n",
        "    print(\"Copying dataset from Drive to local storage...\")\n",
        "    shutil.copytree(drive_backup_path, local_dataset_path)\n",
        "    print(\"Dataset copied to local storage\")\n",
        "    dataset_ready = True\n",
        "else:\n",
        "    print(\"Dataset not found. Please upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    shutil.move('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "    os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "    print(\"Downloading dataset from Kaggle...\")\n",
        "    os.system('kaggle datasets download -d nishat99/bangla-license-plate-detection -p /content')\n",
        "    os.system('unzip -q /content/bangla-license-plate-detection.zip -d /content')\n",
        "\n",
        "    if os.path.exists('/content/Bangla License Plate Dataset'):\n",
        "        shutil.move('/content/Bangla License Plate Dataset', local_dataset_path)\n",
        "        dataset_ready = True\n",
        "\n",
        "        try:\n",
        "            shutil.copytree(local_dataset_path, drive_backup_path)\n",
        "            print(\"Dataset backed up to Drive\")\n",
        "        except:\n",
        "            print(\"Drive backup failed, continuing with local dataset\")\n",
        "\n",
        "if not dataset_ready:\n",
        "    print(\"ERROR: Dataset preparation failed!\")\n",
        "else:\n",
        "    print(f\"Dataset ready at: {local_dataset_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEq6Yf9GvYSG",
        "outputId": "b9d62fa9-8244-4ce7-c0c9-71d9a8c4d6ad"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Convert annotations to YOLO format (same as original)\n",
        "def mask_to_bbox(mask):\n",
        "    \"\"\"Convert binary mask to bounding box coordinates\"\"\"\n",
        "    coords = np.where(mask > 127)\n",
        "    if len(coords[0]) == 0:\n",
        "        return []\n",
        "    y_min, y_max = coords[0].min(), coords[0].max()\n",
        "    x_min, x_max = coords[1].min(), coords[1].max()\n",
        "    return [(x_min, y_min, x_max, y_max)]\n",
        "\n",
        "def bbox_to_yolo_format(bbox, img_width, img_height):\n",
        "    \"\"\"Convert bounding box to YOLO format (normalized)\"\"\"\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "    x_center = (x_min + x_max) / 2.0\n",
        "    y_center = (y_min + y_max) / 2.0\n",
        "    width = x_max - x_min\n",
        "    height = y_max - y_min\n",
        "\n",
        "    x_center_norm = x_center / img_width\n",
        "    y_center_norm = y_center / img_height\n",
        "    width_norm = width / img_width\n",
        "    height_norm = height / img_height\n",
        "\n",
        "    return f\"0 {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}\"\n",
        "\n",
        "def process_dataset_split(split_name, local_dataset_path, yolo_dataset_path):\n",
        "    \"\"\"Process one split with progress tracking\"\"\"\n",
        "    img_folder = os.path.join(local_dataset_path, split_name, 'img')\n",
        "    mask_folder = os.path.join(local_dataset_path, split_name, 'masks')\n",
        "\n",
        "    print(f\"\\nProcessing {split_name}:\")\n",
        "\n",
        "    if not os.path.exists(img_folder) or not os.path.exists(mask_folder):\n",
        "        print(f\"ERROR: Missing folders for {split_name}\")\n",
        "        return 0\n",
        "\n",
        "    yolo_img_dir = os.path.join(yolo_dataset_path, split_name, 'images')\n",
        "    yolo_label_dir = os.path.join(yolo_dataset_path, split_name, 'labels')\n",
        "    os.makedirs(yolo_img_dir, exist_ok=True)\n",
        "    os.makedirs(yolo_label_dir, exist_ok=True)\n",
        "\n",
        "    img_files = [f for f in os.listdir(img_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    print(f\"Found {len(img_files)} images in {split_name}\")\n",
        "\n",
        "    processed_count = 0\n",
        "\n",
        "    for i, img_file in enumerate(img_files):\n",
        "        if i % 500 == 0 and i > 0:\n",
        "            print(f\"  Processed {i}/{len(img_files)} images ({i/len(img_files)*100:.1f}%)\")\n",
        "\n",
        "        try:\n",
        "            mask_name = os.path.splitext(img_file)[0] + '.png'\n",
        "            mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            mask = cv2.imread(mask_path, 0)\n",
        "            if mask is None:\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(img_folder, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img_height, img_width = img.shape[:2]\n",
        "            bboxes = mask_to_bbox(mask)\n",
        "\n",
        "            # Copy image\n",
        "            dst_img_path = os.path.join(yolo_img_dir, img_file)\n",
        "            shutil.copy2(img_path, dst_img_path)\n",
        "\n",
        "            # Create label file\n",
        "            label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "            label_path = os.path.join(yolo_label_dir, label_file)\n",
        "\n",
        "            with open(label_path, 'w') as f:\n",
        "                for bbox in bboxes:\n",
        "                    yolo_line = bbox_to_yolo_format(bbox, img_width, img_height)\n",
        "                    f.write(yolo_line + '\\n')\n",
        "\n",
        "            processed_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"{split_name.upper()} processed: {processed_count} images\")\n",
        "    return processed_count\n",
        "\n",
        "# Check if annotations already converted\n",
        "annotations_complete_flag = os.path.join(yolo_dataset_path, 'annotations_complete.txt')\n",
        "\n",
        "if os.path.exists(annotations_complete_flag):\n",
        "    print(\"Annotations already exist! Skipping annotation generation...\")\n",
        "else:\n",
        "    print(\"Creating YOLO annotations from masks...\")\n",
        "\n",
        "    train_count = process_dataset_split('train', local_dataset_path, yolo_dataset_path)\n",
        "    val_count = process_dataset_split('validation', local_dataset_path, yolo_dataset_path)\n",
        "    test_count = process_dataset_split('test', local_dataset_path, yolo_dataset_path)\n",
        "\n",
        "    total_count = train_count + val_count + test_count\n",
        "    print(f\"\\nANNOTATION CONVERSION COMPLETED!\")\n",
        "    print(f\"Train: {train_count}, Validation: {val_count}, Test: {test_count}\")\n",
        "    print(f\"Total: {total_count} images\")\n",
        "\n",
        "    with open(annotations_complete_flag, 'w') as f:\n",
        "        f.write(f\"Total: {total_count} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v09mOteOvS-l",
        "outputId": "f64124a9-9047-498d-87ce-f53cffd56d76"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Create dataset configuration for YOLOv7\n",
        "dataset_config = {\n",
        "    'path': yolo_dataset_path,\n",
        "    'train': 'train/images',\n",
        "    'val': 'validation/images',\n",
        "    'test': 'test/images',\n",
        "    'nc': 1,\n",
        "    'names': ['license_plate']\n",
        "}\n",
        "\n",
        "config_path = os.path.join(yolo_dataset_path, 'dataset.yaml')\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(dataset_config, f)\n",
        "\n",
        "print(\"Dataset configuration created!\")\n",
        "print(f\"Config saved at: {config_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrns2AO8vNr8",
        "outputId": "d923bfd7-44c9-404e-f12e-857abe58be9d"
      },
      "outputs": [],
      "source": [
        "# Cell 6: YOLOv7m Single-Stage Training with Drive Checkpoint Backup\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import glob\n",
        "\n",
        "# Disable wandb\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "\n",
        "# Make sure we're in the YOLOv7 directory\n",
        "%cd /content/yolov7\n",
        "\n",
        "# COMPREHENSIVE FIX: Patch ALL Python files that use torch.load\n",
        "print(\"Applying comprehensive PyTorch 2.8 compatibility patches...\")\n",
        "\n",
        "def fix_torch_load_in_file(filepath):\n",
        "    \"\"\"Fix torch.load syntax in a single file\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        original_content = content\n",
        "\n",
        "        # Pattern 1: torch.load(weights_only=False, arg, ...) -> torch.load(arg, ..., weights_only=False)\n",
        "        # This handles: torch.load(weights_only=False, f, map_location=...)\n",
        "        content = re.sub(\n",
        "            r'torch\\.load\\(weights_only=False,\\s*([^,\\)]+)(?:,\\s*(.+?))?\\)',\n",
        "            lambda m: f'torch.load({m.group(1)}, {m.group(2)}, weights_only=False)' if m.group(2) else f'torch.load({m.group(1)}, weights_only=False)',\n",
        "            content\n",
        "        )\n",
        "\n",
        "        # Pattern 2: Add weights_only=False to torch.load calls that don't have it\n",
        "        # Match torch.load(...) that doesn't already have weights_only\n",
        "        def add_weights_only(match):\n",
        "            full_call = match.group(0)\n",
        "            if 'weights_only' in full_call:\n",
        "                return full_call\n",
        "            # Find the closing parenthesis\n",
        "            args = match.group(1)\n",
        "            return f'torch.load({args}, weights_only=False)'\n",
        "\n",
        "        content = re.sub(\n",
        "            r'torch\\.load\\(([^)]+)\\)',\n",
        "            add_weights_only,\n",
        "            content\n",
        "        )\n",
        "\n",
        "        if content != original_content:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"  Error processing {filepath}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Find all Python files in YOLOv7 directory\n",
        "python_files = []\n",
        "for root, dirs, files in os.walk('/content/yolov7'):\n",
        "    # Skip __pycache__ and .git directories\n",
        "    dirs[:] = [d for d in dirs if d not in ['__pycache__', '.git', '.github']]\n",
        "    for file in files:\n",
        "        if file.endswith('.py'):\n",
        "            python_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"Scanning {len(python_files)} Python files for torch.load calls...\")\n",
        "\n",
        "patched_files = []\n",
        "for pyfile in python_files:\n",
        "    if fix_torch_load_in_file(pyfile):\n",
        "        # Get relative path for display\n",
        "        rel_path = os.path.relpath(pyfile, '/content/yolov7')\n",
        "        patched_files.append(rel_path)\n",
        "\n",
        "if patched_files:\n",
        "    print(f\"\\n✓ Patched {len(patched_files)} files:\")\n",
        "    for pf in patched_files[:10]:  # Show first 10\n",
        "        print(f\"  - {pf}\")\n",
        "    if len(patched_files) > 10:\n",
        "        print(f\"  ... and {len(patched_files) - 10} more\")\n",
        "else:\n",
        "    print(\"✓ No files needed patching (already fixed)\")\n",
        "\n",
        "# Setup paths\n",
        "model_save_dir = '/content/yolov7_models'\n",
        "drive_checkpoint_dir = '/content/drive/MyDrive/yolov7_checkpoints'\n",
        "\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Check GPU memory and set batch size\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    if gpu_memory < 12:\n",
        "        recommended_batch = 4\n",
        "    elif gpu_memory < 16:\n",
        "        recommended_batch = 8\n",
        "    else:\n",
        "        recommended_batch = 16\n",
        "else:\n",
        "    recommended_batch = 4\n",
        "\n",
        "print(f\"Using batch size: {recommended_batch}\")\n",
        "\n",
        "# VERIFY DATASET AND FIX YAML PATHS\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_imgs = os.path.join(yolo_dataset_path, 'train/images')\n",
        "val_imgs = os.path.join(yolo_dataset_path, 'validation/images')\n",
        "\n",
        "if os.path.exists(train_imgs):\n",
        "    train_count = len([f for f in os.listdir(train_imgs) if f.endswith(('.jpg', '.png'))])\n",
        "    print(f\"✓ Train images: {train_count}\")\n",
        "\n",
        "if os.path.exists(val_imgs):\n",
        "    val_count = len([f for f in os.listdir(val_imgs) if f.endswith(('.jpg', '.png'))])\n",
        "    print(f\"✓ Validation images: {val_count}\")\n",
        "\n",
        "# Fix dataset YAML to use absolute paths\n",
        "print(\"\\nFixing dataset.yaml paths...\")\n",
        "with open(config_path, 'r') as f:\n",
        "    yaml_content = f.read()\n",
        "\n",
        "# Replace relative paths with absolute paths\n",
        "yaml_content = yaml_content.replace('train: train/images', f'train: {train_imgs}')\n",
        "yaml_content = yaml_content.replace('val: validation/images', f'val: {val_imgs}')\n",
        "yaml_content = yaml_content.replace('test: test/images', f'test: {os.path.join(yolo_dataset_path, \"test/images\")}')\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"✓ Updated dataset.yaml with absolute paths\")\n",
        "print(f\"  Train: {train_imgs}\")\n",
        "print(f\"  Val: {val_imgs}\")\n",
        "\n",
        "# CHECK FOR EXISTING CHECKPOINT\n",
        "drive_checkpoint_path = os.path.join(drive_checkpoint_dir, 'last.pt')\n",
        "local_checkpoint_path = os.path.join(model_save_dir, 'single_stage', 'weights', 'last.pt')\n",
        "\n",
        "checkpoint_to_use = None\n",
        "start_epoch = 0\n",
        "\n",
        "# Check for existing checkpoint\n",
        "if os.path.exists(drive_checkpoint_path):\n",
        "    print(f\"\\n✓ Found checkpoint in Drive\")\n",
        "    checkpoint_to_use = drive_checkpoint_path\n",
        "\n",
        "elif os.path.exists(local_checkpoint_path):\n",
        "    print(f\"\\n✓ Found local checkpoint\")\n",
        "    checkpoint_to_use = local_checkpoint_path\n",
        "\n",
        "# If checkpoint exists, use it as starting weights\n",
        "if checkpoint_to_use:\n",
        "    print(f\"Will continue training from: {checkpoint_to_use}\")\n",
        "    weights_to_use = checkpoint_to_use\n",
        "else:\n",
        "    print(\"\\nNo checkpoint found, starting from pretrained weights\")\n",
        "    weights_to_use = 'yolov7.pt'\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if checkpoint_to_use:\n",
        "    print(\"CONTINUING TRAINING FROM CHECKPOINT\")\n",
        "else:\n",
        "    print(\"STARTING NEW TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Build training command - always use fresh training with --exist-ok to continue\n",
        "train_cmd = f\"\"\"python train.py \\\n",
        "  --img 640 \\\n",
        "  --batch {recommended_batch} \\\n",
        "  --epochs 70 \\\n",
        "  --data {config_path} \\\n",
        "  --weights {weights_to_use} \\\n",
        "  --cfg cfg/training/yolov7.yaml \\\n",
        "  --project {model_save_dir} \\\n",
        "  --name single_stage \\\n",
        "  --save_period 10 \\\n",
        "  --exist-ok \\\n",
        "  --workers 4 \\\n",
        "  --device 0\"\"\"\n",
        "\n",
        "print(\"\\nTraining command:\")\n",
        "print(train_cmd)\n",
        "print(\"\\nStarting training...\\n\")\n",
        "\n",
        "# Run training\n",
        "env = os.environ.copy()\n",
        "env['WANDB_DISABLED'] = 'true'\n",
        "env['WANDB_MODE'] = 'disabled'\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    train_cmd,\n",
        "    shell=True,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True,\n",
        "    bufsize=1,\n",
        "    env=env\n",
        ")\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "current_epoch = 0\n",
        "last_progress_line = \"\"\n",
        "last_backup_epoch = -1\n",
        "\n",
        "def backup_to_drive(epoch_num):\n",
        "    \"\"\"Backup checkpoint to Drive\"\"\"\n",
        "    if os.path.exists(local_checkpoint_path):\n",
        "        try:\n",
        "            print(f\"\\nBacking up to Drive (epoch {epoch_num})...\")\n",
        "            shutil.copy2(local_checkpoint_path, drive_checkpoint_path)\n",
        "\n",
        "            local_best = os.path.join(model_save_dir, 'single_stage', 'weights', 'best.pt')\n",
        "            drive_best = os.path.join(drive_checkpoint_dir, 'best.pt')\n",
        "            if os.path.exists(local_best):\n",
        "                shutil.copy2(local_best, drive_best)\n",
        "\n",
        "            print(f\"Backup complete!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Backup failed: {e}\")\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "# Process output\n",
        "for line in process.stdout:\n",
        "    epoch_match = re.search(r'Epoch\\s+(\\d+)/(\\d+)', line)\n",
        "    if epoch_match:\n",
        "        current_epoch = int(epoch_match.group(1))\n",
        "        total_epochs = int(epoch_match.group(2))\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Epoch {current_epoch}/{total_epochs}\")\n",
        "        print('='*60)\n",
        "\n",
        "    if 'Results saved to' in line or 'Epoch completed' in line:\n",
        "        if current_epoch % 5 == 0 and current_epoch != last_backup_epoch and current_epoch > 0:\n",
        "            last_backup_epoch = current_epoch\n",
        "            backup_to_drive(current_epoch)\n",
        "\n",
        "    if any(x in line for x in ['%|', 'it/s', 's/it']) or re.search(r'\\d+/\\d+\\s*\\[', line):\n",
        "        print(f\"\\r{line.strip()[:100]}\", end='', flush=True)\n",
        "        last_progress_line = line\n",
        "\n",
        "    elif any(keyword in line for keyword in ['Class', 'Images', 'P', 'R', 'mAP50',\n",
        "                                              'Results saved', 'val:', 'Model summary',\n",
        "                                              'Best', 'Saving']):\n",
        "        if last_progress_line:\n",
        "            print()\n",
        "            last_progress_line = \"\"\n",
        "        print(line.rstrip())\n",
        "\n",
        "print()\n",
        "return_code = process.wait()\n",
        "\n",
        "# FINAL BACKUP\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL BACKUP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    weights_to_backup = [('last.pt', 'Final checkpoint'), ('best.pt', 'Best model')]\n",
        "\n",
        "    for weight_file, description in weights_to_backup:\n",
        "        local_path = os.path.join(model_save_dir, 'single_stage', 'weights', weight_file)\n",
        "        drive_path = os.path.join(drive_checkpoint_dir, weight_file)\n",
        "\n",
        "        if os.path.exists(local_path):\n",
        "            shutil.copy2(local_path, drive_path)\n",
        "            size_mb = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"{description}: {weight_file} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    for results_file in ['results.csv', 'results.txt']:\n",
        "        results_path = os.path.join(model_save_dir, 'single_stage', results_file)\n",
        "        if os.path.exists(results_path):\n",
        "            shutil.copy2(results_path, os.path.join(drive_checkpoint_dir, results_file))\n",
        "            print(f\"{results_file}\")\n",
        "\n",
        "    print(f\"\\nAll files backed up to Drive\")\n",
        "except Exception as e:\n",
        "    print(f\"Backup error: {e}\")\n",
        "\n",
        "if return_code == 0:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(f\"\\nTraining ended with code {return_code}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Models: /content/yolov7_models/single_stage/weights\")\n",
        "print(\"Drive backup: /content/drive/MyDrive/yolov7_checkpoints\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRKkorW-zReF",
        "outputId": "d234f293-dc08-41d4-ac71-3c06bfed8873"
      },
      "outputs": [],
      "source": [
        "# Cell 6A: MANUAL BACKUP CELL (Run this anytime to backup current progress)\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Paths\n",
        "model_save_dir = '/content/yolov7_models'\n",
        "drive_checkpoint_dir = '/content/drive/MyDrive/yolov7_checkpoints'\n",
        "\n",
        "# Create Drive directory if it doesn't exist\n",
        "os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MANUAL CHECKPOINT BACKUP TO DRIVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Files to backup\n",
        "files_to_backup = {\n",
        "    'last.pt': 'Latest checkpoint',\n",
        "    'best.pt': 'Best model',\n",
        "    'results.csv': 'Training results (CSV)',\n",
        "    'results.txt': 'Training results (TXT)'\n",
        "}\n",
        "\n",
        "# Also backup epoch checkpoints\n",
        "weights_dir = os.path.join(model_save_dir, 'single_stage', 'weights')\n",
        "if os.path.exists(weights_dir):\n",
        "    epoch_files = [f for f in os.listdir(weights_dir) if f.startswith('epoch') and f.endswith('.pt')]\n",
        "    for ef in epoch_files:\n",
        "        files_to_backup[ef] = f'Periodic checkpoint'\n",
        "\n",
        "backup_count = 0\n",
        "for filename, description in files_to_backup.items():\n",
        "    if filename in ['results.csv', 'results.txt']:\n",
        "        local_path = os.path.join(model_save_dir, 'single_stage', filename)\n",
        "    else:\n",
        "        local_path = os.path.join(model_save_dir, 'single_stage', 'weights', filename)\n",
        "\n",
        "    drive_path = os.path.join(drive_checkpoint_dir, filename)\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        try:\n",
        "            shutil.copy2(local_path, drive_path)\n",
        "            size_mb = os.path.getsize(local_path) / (1024*1024)\n",
        "            print(f\"✓ {description}: {filename} ({size_mb:.1f} MB)\")\n",
        "            backup_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to backup {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"⚠ Not found: {filename}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Backup complete! {backup_count} files saved to Drive\")\n",
        "print(f\"Location: {drive_checkpoint_dir}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# List all files in Drive backup location\n",
        "print(\"\\nFiles in Drive backup:\")\n",
        "if os.path.exists(drive_checkpoint_dir):\n",
        "    drive_files = sorted(os.listdir(drive_checkpoint_dir))\n",
        "    for f in drive_files:\n",
        "        fpath = os.path.join(drive_checkpoint_dir, f)\n",
        "        size_mb = os.path.getsize(fpath) / (1024*1024)\n",
        "        mtime = datetime.fromtimestamp(os.path.getmtime(fpath))\n",
        "        print(f\"  • {f} ({size_mb:.1f} MB) - {mtime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "else:\n",
        "    print(\"  No files found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JIwXZpku4B8",
        "outputId": "bfbdf743-b868-4d81-c15e-5c1058af33d2"
      },
      "outputs": [],
      "source": [
        "# Cell 7: YOLOv7 Evaluation (adapted for YOLOv7)\n",
        "import torch\n",
        "from utils.general import non_max_suppression, scale_coords\n",
        "from utils.torch_utils import select_device\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "def load_yolov7_model(weights_path):\n",
        "    \"\"\"Load YOLOv7 model\"\"\"\n",
        "    device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
        "    model = attempt_load(weights_path, map_location=device)\n",
        "    model.eval()\n",
        "    return model, device\n",
        "\n",
        "def detect_yolov7(model, device, img_path, conf_thresh=0.5, img_size=640):\n",
        "    \"\"\"Run YOLOv7 detection\"\"\"\n",
        "    from utils.datasets import LoadImages\n",
        "    from utils.general import check_img_size\n",
        "\n",
        "    # Ensure img_size is compatible with model stride\n",
        "    stride = int(model.stride.max())\n",
        "    img_size = check_img_size(img_size, s=stride)\n",
        "\n",
        "    # Load image\n",
        "    img = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h0, w0 = img.shape[:2]\n",
        "\n",
        "    # Resize and pad\n",
        "    r = img_size / max(h0, w0)\n",
        "    if r != 1:\n",
        "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
        "        img_resized = cv2.resize(img_rgb, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
        "    else:\n",
        "        img_resized = img_rgb\n",
        "\n",
        "    # Pad to img_size\n",
        "    dh, dw = img_size - img_resized.shape[0], img_size - img_resized.shape[1]\n",
        "    dh, dw = dh // 2, dw // 2\n",
        "    img_padded = cv2.copyMakeBorder(img_resized, dh, img_size - img_resized.shape[0] - dh,\n",
        "                                     dw, img_size - img_resized.shape[1] - dw,\n",
        "                                     cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "\n",
        "    # Convert to tensor\n",
        "    img_input = img_padded.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_input = np.ascontiguousarray(img_input)\n",
        "    img_input = torch.from_numpy(img_input).to(device)\n",
        "    img_input = img_input.float()\n",
        "    img_input /= 255.0\n",
        "\n",
        "    if img_input.ndimension() == 3:\n",
        "        img_input = img_input.unsqueeze(0)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        pred = model(img_input, augment=False)[0]\n",
        "\n",
        "    # NMS\n",
        "    pred = non_max_suppression(pred, conf_thresh, 0.45, classes=None, agnostic=False)\n",
        "\n",
        "    # Process detections\n",
        "    boxes = []\n",
        "    if pred[0] is not None and len(pred[0]):\n",
        "        det = pred[0]\n",
        "        # Scale coordinates back to original image\n",
        "        det[:, :4] = scale_coords(img_input.shape[2:], det[:, :4], img.shape).round()\n",
        "\n",
        "        for *xyxy, conf, cls in det:\n",
        "            x1, y1, x2, y2 = [float(x) for x in xyxy]\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    return boxes\n",
        "\n",
        "def load_ground_truth_boxes(label_path, img_width, img_height):\n",
        "    \"\"\"Load ground truth boxes from YOLO format label file\"\"\"\n",
        "    boxes = []\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split()\n",
        "                if len(parts) == 5:\n",
        "                    try:\n",
        "                        _, x_center, y_center, width, height = map(float, parts)\n",
        "\n",
        "                        x_center_abs = x_center * img_width\n",
        "                        y_center_abs = y_center * img_height\n",
        "                        width_abs = width * img_width\n",
        "                        height_abs = height * img_height\n",
        "\n",
        "                        x1 = x_center_abs - width_abs / 2\n",
        "                        y1 = y_center_abs - height_abs / 2\n",
        "                        x2 = x_center_abs + width_abs / 2\n",
        "                        y2 = y_center_abs + height_abs / 2\n",
        "\n",
        "                        boxes.append([x1, y1, x2, y2])\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "    return boxes\n",
        "\n",
        "def calculate_box_iou(box1, box2):\n",
        "    \"\"\"Calculate IoU between two bounding boxes\"\"\"\n",
        "    x1_1, y1_1, x2_1, y2_1 = box1\n",
        "    x1_2, y1_2, x2_2, y2_2 = box2\n",
        "\n",
        "    x1_i = max(x1_1, x1_2)\n",
        "    y1_i = max(y1_1, y1_2)\n",
        "    x2_i = min(x2_1, x2_2)\n",
        "    y2_i = min(y2_1, y2_2)\n",
        "\n",
        "    if x2_i <= x1_i or y2_i <= y1_i:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = (x2_i - x1_i) * (y2_i - y1_i)\n",
        "    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    if union_area == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return intersection_area / union_area\n",
        "\n",
        "def evaluate_yolov7(model, device, split_name, conf_thresh=0.5):\n",
        "    \"\"\"Comprehensive evaluation with same metrics\"\"\"\n",
        "    img_dir = f'{yolo_dataset_path}/{split_name}/images'\n",
        "    label_dir = f'{yolo_dataset_path}/{split_name}/labels'\n",
        "\n",
        "    img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    tp, fp, fn, tn = 0, 0, 0, 0\n",
        "    iou_scores = []\n",
        "\n",
        "    print(f\"Evaluating {len(img_files)} images...\")\n",
        "\n",
        "    for i, img_file in enumerate(img_files):\n",
        "        if i % 500 == 0 and i > 0:\n",
        "            print(f\"Progress: {i}/{len(img_files)} ({i/len(img_files)*100:.1f}%)\")\n",
        "\n",
        "        img_path = f'{img_dir}/{img_file}'\n",
        "        label_path = f'{label_dir}/{os.path.splitext(img_file)[0]}.txt'\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        gt_boxes = load_ground_truth_boxes(label_path, w, h)\n",
        "        has_gt = len(gt_boxes) > 0\n",
        "\n",
        "        pred_boxes = detect_yolov7(model, device, img_path, conf_thresh)\n",
        "        has_pred = len(pred_boxes) > 0\n",
        "\n",
        "        if has_gt and has_pred:\n",
        "            max_iou = 0\n",
        "            for gt_box in gt_boxes:\n",
        "                for pred_box in pred_boxes:\n",
        "                    iou = calculate_box_iou(gt_box, pred_box)\n",
        "                    max_iou = max(max_iou, iou)\n",
        "\n",
        "            iou_scores.append(max_iou)\n",
        "            if max_iou > 0.5:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "        elif has_gt and not has_pred:\n",
        "            fn += 1\n",
        "            iou_scores.append(0)\n",
        "        elif not has_gt and has_pred:\n",
        "            fp += 1\n",
        "            iou_scores.append(0)\n",
        "        else:\n",
        "            tn += 1\n",
        "            iou_scores.append(1)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    accuracy = (tp + tn) / len(img_files)\n",
        "    avg_iou = sum(iou_scores) / len(iou_scores)\n",
        "    binary_detection = sum(1 for iou in iou_scores if iou > 0.7) / len(iou_scores)\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'accuracy': accuracy,\n",
        "        'avg_iou': avg_iou,\n",
        "        'binary_detection': binary_detection,\n",
        "        'iou_scores': iou_scores\n",
        "    }\n",
        "\n",
        "# Load the trained model\n",
        "best_model_path = os.path.join(model_save_dir, 'single_stage', 'weights', 'best.pt')\n",
        "print(f\"Loading model from: {best_model_path}\")\n",
        "trained_model, device = load_yolov7_model(best_model_path)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nEvaluating YOLOv7m model...\")\n",
        "val_results = evaluate_yolov7(trained_model, device, 'validation')\n",
        "test_results = evaluate_yolov7(trained_model, device, 'test')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"YOLOv7m SINGLE-STAGE RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nVALIDATION RESULTS:\")\n",
        "print(f\"IoU: {val_results['avg_iou']:.4f}\")\n",
        "print(f\"Precision: {val_results['precision']:.4f}\")\n",
        "print(f\"Recall: {val_results['recall']:.4f}\")\n",
        "print(f\"F1-Score: {val_results['f1_score']:.4f}\")\n",
        "print(f\"Binary Detection (IoU>0.7): {val_results['binary_detection']:.4f}\")\n",
        "\n",
        "print(\"\\nTEST RESULTS:\")\n",
        "print(f\"IoU: {test_results['avg_iou']:.4f}\")\n",
        "print(f\"Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"F1-Score: {test_results['f1_score']:.4f}\")\n",
        "print(f\"Binary Detection (IoU>0.7): {test_results['binary_detection']:.4f}\")\n",
        "print(f\"Boundary Box Accuracy: {test_results['binary_detection']*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1CuDuXBuwlS",
        "outputId": "2be07bac-2c30-4fde-d0d4-a60873ff3a33"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Training Visualization for YOLOv7\n",
        "# YOLOv7 may save results as results.txt or results.csv\n",
        "results_files = [\n",
        "    f'{model_save_dir}/single_stage/results.csv',\n",
        "    f'{model_save_dir}/single_stage/results.txt'\n",
        "]\n",
        "\n",
        "results_csv = None\n",
        "for rf in results_files:\n",
        "    if os.path.exists(rf):\n",
        "        results_csv = rf\n",
        "        break\n",
        "\n",
        "if results_csv:\n",
        "    # Try to read as CSV first\n",
        "    try:\n",
        "        if results_csv.endswith('.csv'):\n",
        "            data = pd.read_csv(results_csv)\n",
        "        else:\n",
        "            # YOLOv7 results.txt is space-separated\n",
        "            data = pd.read_csv(results_csv, delim_whitespace=True)\n",
        "\n",
        "        # Strip whitespace from column names\n",
        "        data.columns = data.columns.str.strip()\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Create epoch column if not present\n",
        "        if 'epoch' not in data.columns:\n",
        "            epochs = range(1, len(data) + 1)\n",
        "        else:\n",
        "            epochs = data['epoch']\n",
        "\n",
        "        # Box Loss - adapt column names for YOLOv7\n",
        "        box_loss_cols = ['train/box_loss', 'box_loss', 'box']\n",
        "        val_box_loss_cols = ['val/box_loss', 'val_box_loss']\n",
        "\n",
        "        for col in box_loss_cols:\n",
        "            if col in data.columns:\n",
        "                axes[0,0].plot(epochs, data[col], 'b-', label='Train Box Loss')\n",
        "                break\n",
        "\n",
        "        for col in val_box_loss_cols:\n",
        "            if col in data.columns:\n",
        "                axes[0,0].plot(epochs, data[col], 'r-', label='Val Box Loss')\n",
        "                break\n",
        "\n",
        "        axes[0,0].set_title('Box Loss')\n",
        "        axes[0,0].set_xlabel('Epoch')\n",
        "        axes[0,0].set_ylabel('Loss')\n",
        "        axes[0,0].legend()\n",
        "        axes[0,0].grid(True)\n",
        "\n",
        "        # mAP@0.5 - adapt column names for YOLOv7\n",
        "        map_cols = ['metrics/mAP_0.5', 'mAP@0.5', 'mAP_0.5', 'val/mAP_0.5']\n",
        "        for col in map_cols:\n",
        "            if col in data.columns:\n",
        "                axes[0,1].plot(epochs, data[col], 'g-', label='mAP@0.5')\n",
        "                axes[0,1].set_title('mAP@0.5')\n",
        "                axes[0,1].set_xlabel('Epoch')\n",
        "                axes[0,1].set_ylabel('mAP')\n",
        "                axes[0,1].legend()\n",
        "                axes[0,1].grid(True)\n",
        "                break\n",
        "\n",
        "        # Precision & Recall\n",
        "        prec_cols = ['metrics/precision', 'precision', 'P', 'val/precision']\n",
        "        recall_cols = ['metrics/recall', 'recall', 'R', 'val/recall']\n",
        "\n",
        "        prec_col = None\n",
        "        recall_col = None\n",
        "\n",
        "        for col in prec_cols:\n",
        "            if col in data.columns:\n",
        "                prec_col = col\n",
        "                break\n",
        "        for col in recall_cols:\n",
        "            if col in data.columns:\n",
        "                recall_col = col\n",
        "                break\n",
        "\n",
        "        if prec_col and recall_col:\n",
        "            axes[1,0].plot(epochs, data[prec_col], 'purple', label='Precision')\n",
        "            axes[1,0].plot(epochs, data[recall_col], 'brown', label='Recall')\n",
        "            axes[1,0].set_title('Precision & Recall')\n",
        "            axes[1,0].set_xlabel('Epoch')\n",
        "            axes[1,0].set_ylabel('Score')\n",
        "            axes[1,0].legend()\n",
        "            axes[1,0].grid(True)\n",
        "\n",
        "        # Final metrics comparison\n",
        "        metrics = ['IoU', 'Precision', 'Recall', 'F1', 'Binary Det']\n",
        "        test_scores = [test_results['avg_iou'], test_results['precision'],\n",
        "                       test_results['recall'], test_results['f1_score'],\n",
        "                       test_results['binary_detection']]\n",
        "\n",
        "        axes[1,1].bar(metrics, test_scores, alpha=0.7, color=['blue', 'green', 'red', 'orange', 'purple'])\n",
        "        axes[1,1].set_title('Final Test Performance')\n",
        "        axes[1,1].set_xticklabels(metrics, rotation=45)\n",
        "        axes[1,1].set_ylim([0, 1])\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{model_save_dir}/single_stage/training_metrics.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nTraining Summary:\")\n",
        "        print(f\"Total epochs: {len(data)}\")\n",
        "\n",
        "        # Find best mAP\n",
        "        for col in map_cols:\n",
        "            if col in data.columns:\n",
        "                print(f\"Best mAP@0.5: {data[col].max():.4f}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Final Test IoU: {test_results['avg_iou']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading results file: {e}\")\n",
        "        print(\"Results file exists but couldn't be parsed\")\n",
        "else:\n",
        "    print(f\"Results file not found in: {model_save_dir}/single_stage/\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"YOLOv7m single-stage training completed!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu9tne8auq-V"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Export and Download Models\n",
        "import shutil\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Specify the folder path you want to zip\n",
        "folder_path = '/content/yolov7_models'\n",
        "zip_name = 'yolov7model'\n",
        "\n",
        "# Create a zip file of the folder\n",
        "shutil.make_archive(zip_name, 'zip', folder_path)\n",
        "\n",
        "# Save to Google Drive\n",
        "drive_path = '/content/drive/MyDrive/yolov7model.zip'\n",
        "shutil.move(f'{zip_name}.zip', drive_path)\n",
        "print(f\"Saved to Google Drive: {drive_path}\")\n",
        "\n",
        "# Also download to your local machine (optional - uncomment if needed)\n",
        "# shutil.make_archive(zip_name, 'zip', folder_path)\n",
        "# files.download(f'{zip_name}.zip')\n",
        "print(f\"Downloaded {zip_name}.zip to your computer successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
